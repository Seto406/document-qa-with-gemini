# AI-Powered Document Q&A System with Gemini ðŸ¤–

This project is a complete Retrieval-Augmented Generation (RAG) pipeline built in Python. It allows a user to ask questions in natural language about a custom set of documents and receive context-aware answers generated by Google's Gemini AI model.

---
### Workflow

1.  **Document Loading:** The pipeline loads multiple PDF documents from a local directory.
2.  **Text Chunking:** The documents are split into smaller, overlapping text chunks to prepare them for embedding.
3.  **Embedding & Storage:** Each text chunk is converted into a numerical representation (an embedding) using Google's `embedding-001` model. These embeddings are then stored in a `ChromaDB` vector database.
4.  **Retrieval & Generation:** When a user asks a question, the system searches the vector database to find the most relevant text chunks. These chunks, along with the original question, are passed to the `gemini-1.5-flash` model, which generates a final, context-aware answer.

---
### Technologies Used

* **AI/ML:** Google Gemini, LangChain
* **Language:** Python
* **Libraries:** `langchain-google-genai`, `chromadb`, `pypdf`
* **Database:** ChromaDB (Vector Database)

---
### How to Run

1.  Clone the repository.
2.  Create and activate a virtual environment.
3.  Install dependencies: `pip install -r requirements.txt` *(Note: you will need to create this file)*.
4.  Create a `.env` file and add your `GOOGLE_API_KEY`.
5.  Place your PDF documents into the `documents` folder.
6.  Run the script: `python app.py`